{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"HvFni82aAGg4","executionInfo":{"status":"ok","timestamp":1715122200404,"user_tz":-180,"elapsed":2668,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["import numpy as np\n","from scipy.integrate import odeint\n","import matplotlib.pyplot as plt\n","import plotly.graph_objects as go\n","import numpy as np\n","from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eemLPhl_Sn2"},"outputs":[],"source":["# Importing data and add it to an array from one drive\n","\n","#path_1 = \"/content/drive/MyDrive/Colab Notebooks/MECH 798M/Mech 798M final Project /Machine Learning Data  - Longitudinal tension, Longitudinal compression, Transverse tension, Transverse compression, In-plane shear.xlsx\"\n","path_1 = \"/content/drive/MyDrive/Colab Notebooks/MECH 798M/Mech 798M final Project /Machine Learning Data .xlsx\"\n","\n","\n","df1 = pd.read_excel(path_1 )\n","\n","# Display the first few rows of the DataFrame\n","print('The data in the  data array is' ,df1.head(1))\n","\n","data_array_1 = df1.values\n","print('The shape of the data array is ',data_array_1.shape)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLLYr1BQbxzh"},"outputs":[],"source":["# One-hot Encoding of the data Input Variables\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","#  Data Frame\n","data = {\n","   'Fibre': data_array_1[:,1],\n","   'Resin': data_array_1[:,2],\n","   'Test': data_array_1[:,3],\n","   'Cure Cycle': data_array_1[:,4],\n","   'Condition': data_array_1[:,5],\n","   'Orientation': data_array_1[:,6],\n","  # '#Plies in Laminate': data_array_1[:,7],  removed feature that worsens accuracy\n","   'Modulus': data_array_1[:,-1],\n","\n","}\n","data['Modulus'] = pd.to_numeric(data['Modulus'], errors='coerce')  # turning data into numerical data\n","\n","\n","df = pd.DataFrame(data)\n","\n","# Perform one-hot encoding\n","encoded_df = pd.get_dummies(df, columns=['Fibre', 'Resin', 'Test', 'Cure Cycle', 'Condition',\n","                                          'Orientation'])\n","# Convert encoded DataFrame to NumPy array\n","encoded_array = encoded_df.values\n","\n","# Boolean matrix\n","encoded_array = np.array(encoded_array)\n","\n","# Display the encoded DataFrame\n","print('Enocoded df',encoded_df)\n","\n","# Display the array\n","print('Enocoded array',encoded_array.shape)\n","\n","# Display the shape of the encoded_array array\n","print('Enocoded array shape',encoded_array)"]},{"cell_type":"markdown","source":["**Visualization of encoded data and collecting it for external analysis**"],"metadata":{"id":"V5AAp1SrI5fI"}},{"cell_type":"code","source":["# Data Visualization using Heat map to see features correlations.\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Concatenate the output column vector to the one-hot encoded data matrix\n","data_array_1_df = pd.DataFrame(data['Modulus'])\n","\n","data_with_output = pd.concat([encoded_df, data_array_1_df], axis=1)\n","\n","data_with_output = data_with_output.iloc[:,:-1]\n","\n","# Calculate the correlation matrix\n","correlation_matrix = data_with_output.corr()\n","\n","\n","# Plot correlation matrix as a heatmap\n","plt.figure(figsize=(20, 16))\n","\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","# Define custom color palette\n","num_variables = len(correlation_matrix )\n","colors = plt.cm.viridis(np.linspace(0, 1, num_variables))  # Using Viridis color map\n","\n","# Set the desired figure size\n","plt.figure(figsize=(50, 20))\n","\n","# Plot correlations as a bar chart with custom colors\n","bars = correlation_matrix .plot(kind='bar', color=colors, width=0.8)\n","\n","plt.title('Correlation between Input Variables and Output Variable')\n","plt.xlabel('Input Variables')\n","plt.ylabel('Correlation')\n","plt.xticks(rotation=45, ha='right')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","# Increase size of the data area\n","ax = plt.gca()\n","ax.set_position([1, 1, 1.5, 1.5])\n","\n","# Move legend outside the plot\n","plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n","\n","plt.show()\n","\n","\n"],"metadata":{"id":"wlb2rh17Ayok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data visualization of one composite compossed of fiber  \"AS4\" and resin \"MTM45-1\"\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [0:363, 1], data_array_1  [0:363,-1], label='Composite 1 Fiber vs Youngs Modulus', alpha=0.6)\n","plt.xlabel('Fiber')\n","plt.ylabel('Modulus ')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [0:363, 2], data_array_1  [0:363,-1], label='Composite 1 Resin vs Youngs Modulus', alpha=0.6)\n","plt.xlabel('Resin')\n","plt.ylabel('Modulus ')\n","\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [0:363, 3], data_array_1  [0:363,-1], label='Composite 1 Test vs Youngs Modulus', alpha=0.6)\n","plt.xlabel('Test')\n","plt.ylabel('Modulus ')\n","\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [0:363, 5], data_array_1  [0:363,-1], label='Composite 1 Condition vs Youngs Modulus', alpha=0.6)\n","plt.xlabel('Condition')\n","plt.ylabel('Modulus ')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [0:363, 6], data_array_1  [0:363,-1], label='Composite 1 Orientation vs Youngs Modulus', alpha=0.6)\n","\n","plt.xlabel('Orientation')\n","plt.ylabel('Modulus ')\n","\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n"],"metadata":{"id":"-nYFkRo4ObKu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI21NkTqkbPb"},"outputs":[],"source":["# Data visualization of all composites\n","import matplotlib.pyplot as plt\n","\n","\n","# Create scatter plot\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 1], data_array_1  [:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Fiber type ')\n","plt.ylabel('Modulus ')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 2], data_array_1  [:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Resin type ')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1 [:, 3], data_array_1[:, -1], label='Input 4 vs Output', alpha=0.6)\n","plt.xlabel('Test')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 4], data_array_1[:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Cure Cycle')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 5], data_array_1[:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Environmental Condition')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 6], data_array_1[:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Fiber orientation')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 7], data_array_1[:, -1], label='Input 3 vs Output', alpha=0.6)\n","plt.xlabel('Number of plies ')\n","plt.ylabel('Modulus')\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(data_array_1  [:, 2], data_array_1[:, 4], label='Resin type vs Cure cycle', alpha=0.6)\n","plt.xlabel('Resin ')\n","plt.ylabel('cure cycle')\n","\n","# Labeling the plot\n","plt.title('Scatter Plot of Inputs vs Output')\n","plt.xlabel('Input Values')\n","plt.ylabel('Output Values')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtwEviDOhJ1f"},"outputs":[],"source":["# Adding the one hot encoded data to view in a file like excel\n","import pandas as pd\n","\n","# Convert the array to a DataFrame\n","df = pd.DataFrame(encoded_df)\n","\n","# Define the file path for the Excel file\n","file_path = '/content/drive/MyDrive/Colab Notebooks/MECH 798M/Mech 798M final Project /Viewing.xlsx'  # Change the file path as needed\n","\n","# Export the DataFrame to an Excel file\n","df.to_excel(file_path, index=False)\n","\n","print(\"Array data of one hot encoded data exported to Excel file:\", file_path)\n"]},{"cell_type":"markdown","source":["**Model Training**"],"metadata":{"id":"xyKi-_NtJfsP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNW4aPdXQIWW"},"outputs":[],"source":["# Shuffling Data then splitting\n","import numpy as np\n","\n","\n","# Shuffle the data matrix along the rows\n","np.random.shuffle(encoded_array)\n","\n","# Define proportions for train, validation, and test sets\n","train_ratio = 0.6\n","\n","val_ratio = 0.2\n","\n","test_ratio = 0.2\n","\n","# Calculate sizes for train, validation, and test sets\n","num_samples = encoded_array.shape[0]\n","\n","train_size = int(train_ratio * num_samples)\n","\n","val_size = int(val_ratio * num_samples)\n","\n","# Split the shuffled data matrix into train, validation, and test sets\n","train_data = encoded_array[:train_size, :]\n","\n","val_data = encoded_array[train_size:train_size + val_size, :]\n","\n","test_data = encoded_array[train_size + val_size:, :]\n","\n","# Display the sizes of train, validation, and test sets\n","print(\"Train Data Size:\", train_data.shape)\n","\n","print(\"Validation Data Size:\", val_data.shape)\n","\n","print(\"Test Data Size:\", test_data.shape)\n","\n","# Display the train, validation, and test data sets\n","\n","print(\"Train Data Size:\", train_data)\n","\n","print(\"Validation Data Size:\", val_data)\n","\n","print(\"Test Data Size:\", test_data)"]},{"cell_type":"markdown","source":["**Linear Regression : Linear Regression : Linear Regression : Linear Regression : Linear Regression**"],"metadata":{"id":"W5VZSmqvksRg"}},{"cell_type":"code","source":["# Linear  Regression Model\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","# Output data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Modle\n","model = LinearRegression()\n","\n","# Fit the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Predict on the testing data\n","y_pred_train_LR = model.predict(X_train) # Predict train set\n","\n","y_pred_test_LR = model.predict(X_test) # Predict test set\n","\n","\n","# Calculate Mean Squared Error\n","mse_train  = mean_squared_error(y_train, y_pred_train_LR) # Calculate Mean Squared Error for train set\n","\n","mse_test = mean_squared_error(y_test, y_pred_test_LR) # Calculate Mean Squared Error for tet set\n","\n","print(\"Mean Squared Error for train :\", mse_train ,\"Mean Squared Errorfor test:\", mse_test)\n","\n","\n","\n","\n"],"metadata":{"id":"y1khic9yjio-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualization of Linear regression results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","# Scatter plot\n","plt.figure(figsize=(10, 6))\n","plt.scatter(y_test,  y_pred_test_LR, color='blue', label='Data points')\n","\n","# Line indicating perfect predictions\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect predictions')\n","\n","plt.title('Linear  Regression Model Actual vs Predicted')\n","plt.xlabel('Actual Values (y_test)')\n","plt.ylabel('Predicted Values (y_pred)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"z8F2VWekusRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0oTC7dmpL1ll"},"source":["**Ridge Regression : Ridge Regression : Ridge Regression : Ridge Regression : Ridge Regression : Ridge Regression**"]},{"cell_type":"markdown","metadata":{"id":"Y-U41QLPQQOc"},"source":["Hyperparameter tuning of  Ridge regression model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHEGHYpKQNAK"},"outputs":[],"source":["\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","# Output data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Define alpha values to try\n","alphas = np.linspace(0,10, 1000 )  # Try 101 alpha values from 0 to 100\n","\n","# Initialize lists to store MSE values for training and test sets\n","train_mse_values = []\n","Validation_mse_values = []\n","\n","# Loop over each alpha value\n","for alpha in alphas:\n","    # Create Ridge Regression model with current alpha value\n","    ridge_model = Ridge(alpha=alpha)\n","\n","    # Fit the model to the training data\n","    ridge_model.fit(X_train, y_train)\n","\n","    # Predict on training and test sets\n","    y_train_pred = ridge_model.predict(X_train)\n","    y_test_pred = ridge_model.predict(X_validation)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred)\n","    test_mse = mean_squared_error(y_validation, y_test_pred)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","    print(f\"Alpha: {alpha}, Train MSE: {train_mse}, Validation MSE: {test_mse}\")\n","\n","# Find the alpha that minimizes the test MSE\n","best_alpha = alphas[np.argmin(Validation_mse_values)]\n","print(f\"Best alpha: {best_alpha}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PahPF3XPTtLY","executionInfo":{"status":"aborted","timestamp":1715122234702,"user_tz":-180,"elapsed":38027,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["\n","# Define Ridge Regression solvers to try\n","solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n","\n","# Initialize lists to store MSE values for training and test sets\n","train_mse_values = []\n","Validation_mse_values = []\n","\n","# Loop over each solver\n","for solver in solvers:\n","    # Create Ridge Regression model with current solver\n","    ridge_model = Ridge(alpha=best_alpha, solver=solver)\n","\n","    # Fit the model to the training data\n","    ridge_model.fit(X_train, y_train)\n","\n","    # Predict on training and test sets\n","    y_train_pred = ridge_model.predict(X_train)\n","    y_test_pred = ridge_model.predict(X_test)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred)\n","    test_mse = mean_squared_error(y_test, y_test_pred)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","    print(f\"Solver: {solver}, Train MSE: {train_mse}, Test MSE: {test_mse}\")\n","\n","# Find the solver that minimizes the test MSE\n","best_solver = solvers[np.argmin(Validation_mse_values)]\n","plt.figure(figsize=(50, 20))\n","\n","# Visualize\n","plt.plot(Validation_mse_values)\n","\n","print(f\"Best solver: {best_solver}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQIM6uoYPKv3","executionInfo":{"status":"aborted","timestamp":1715122234702,"user_tz":-180,"elapsed":38025,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["# Ridge Regression Model\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","\n","# Train Ridge regression model\n","alpha =  best_alpha # Regularization strength (hyperparameter)\n","\n","ridge_model = Ridge(alpha=alpha, solver='auto', max_iter=None, tol=0.001)\n","\n","ridge_model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred_train_RR = ridge_model.predict(X_train)\n","\n","y_pred_test_RR = ridge_model.predict(X_test)\n","\n","# Calculate Mean Squared Error\n","mse_train  = mean_squared_error(y_train, y_pred_train_RR  ) # Calculate Mean Squared Error for train set\n","\n","mse_test = mean_squared_error(y_test, y_pred_test_RR) # Calculate Mean Squared Error for test set\n","\n","# Print model results\n","print(\"Mean Squared Error for train :\", mse_train ,\"Mean Squared Errorfor test:\", mse_test)\n","\n","# Print model coefficients (optional)\n","print(\"Model Coefficients:\", ridge_model.coef_)\n"]},{"cell_type":"code","source":["# Visualization of  Ridge regression results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","# Scatter plot\n","plt.figure(figsize=(10, 6))\n","plt.scatter(y_test,   y_pred_test_RR, color='blue', label='Data points')\n","\n","# Line indicating perfect predictions\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect predictions')\n","\n","plt.title('Ridge regression results Actual vs Predicted')\n","plt.xlabel('Actual Values (y_test)')\n","plt.ylabel('Predicted Values (y_pred)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"R5aA-CoYyTUN","executionInfo":{"status":"aborted","timestamp":1715122234984,"user_tz":-180,"elapsed":2,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWfkc_L1LXsS"},"source":["**Random ForestRegressor : Random ForestRegressor : Random ForestRegressor  :\n","Random ForestRegressor : Random ForestRegressor**"]},{"cell_type":"markdown","source":["Hyperparameter tuning of Random ForestRegressor model"],"metadata":{"id":"jS_OFWZZNqfy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iNQ0jPtVpCX","executionInfo":{"status":"aborted","timestamp":1715122234985,"user_tz":-180,"elapsed":2,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["# Random ForestRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","\n","\n","# Initialize lists to store MSE values for training and test sets\n","train_mse_values = []\n","\n","Validation_mse_values = []\n","\n","estimators = np.arange(1, 201)\n","\n","\n","# Loop over each n_estimators value\n","for n_estimators in estimators:\n","\n","    # Initialize Random Forest Regressor\n","    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, max_depth=10, random_state=42)\n","\n","    # Train the model\n","    rf_regressor.fit(X_train, y_train)\n","\n","    # Predict on training and test sets\n","\n","    y_train_pred_RFR = rf_regressor.predict(X_train)\n","    y_test_pred_RFR = rf_regressor.predict(X_validation)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred_RFR)\n","    test_mse = mean_squared_error(y_validation, y_test_pred_RFR)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","\n","    print(f\"N_estimators: {n_estimators}, Train MSE: {train_mse}, Validation MSE: {test_mse}\")\n","\n","# Find the alpha that minimizes the test MSE\n","best_estimators = estimators[np.argmin(Validation_mse_values)]\n","\n","plt.figure(figsize=(50, 20))\n","\n","# Visualization\n","plt.plot(Validation_mse_values)\n","\n","print(f\"Best estimators: {best_estimators}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIl3c0XcF5J7","executionInfo":{"status":"aborted","timestamp":1715122234985,"user_tz":-180,"elapsed":2,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["# Loop over each max_depth value\n","train_mse_values = []\n","Validation_mse_values = []\n","depth = np.arange(1, 101)\n","\n","\n","for max_depth in depth:\n","\n","    # Initialize Random Forest Regressor\n","    rf_regressor = RandomForestRegressor(n_estimators=best_estimators, max_depth=max_depth, random_state=42)\n","\n","    # Train the model\n","    rf_regressor.fit(X_train, y_train)\n","\n","    # Predict on training and test sets\n","\n","    y_train_pred_RFR = rf_regressor.predict(X_train)\n","    y_test_pred_RFR = rf_regressor.predict(X_validation)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred_RFR)\n","    test_mse = mean_squared_error(y_validation, y_test_pred_RFR)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","\n","    print(f\"Max_depth: { max_depth}, Train MSE: {train_mse}, Validation MSE: {test_mse}\")\n","\n","# Find the alpha that minimizes the test MSE\n","best_depth = estimators[np.argmin(Validation_mse_values)]\n","plt.figure(figsize=(50, 20))\n","plt.plot(Validation_mse_values)\n","print(f\"Best depth: {best_depth}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uijkVynztMBC","executionInfo":{"status":"aborted","timestamp":1715122234986,"user_tz":-180,"elapsed":38300,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["# Random ForestRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","# Input data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Initialize Random Forest Regressor\n","rf_regressor = RandomForestRegressor(n_estimators=best_estimators, max_depth=best_depth, random_state=42)\n","\n","# Train the model\n","rf_regressor.fit(X_train, y_train)\n","\n","\n","\n","# Predict on the test set\n","y_pred_train_RFR = rf_regressor.predict(X_train) # Predict train set\n","\n","y_pred_test_RFR = rf_regressor.predict(X_test) # Predict train set\n","\n","# Calculate Mean Squared Error\n","mse_train  = mean_squared_error(y_train, y_pred_train_RFR ) # Calculate Mean Squared Error for train set\n","mse_test = mean_squared_error(y_test, y_pred_test_RFR) # Calculate Mean Squared Error for test set\n","\n","print(\"Mean Squared Error for train :\", mse_train ,\"Mean Squared Errorfor test:\", mse_test)"]},{"cell_type":"code","source":["# Visualization of Random ForestRegressor results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","# Scatter plot\n","plt.figure(figsize=(10, 6))\n","plt.scatter(y_test,   y_pred_test_RFR, color='blue', label='Data points')\n","\n","# Line indicating perfect predictions\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect predictions')\n","\n","plt.title('Random ForestRegressor results Actual vs Predicted')\n","plt.xlabel('Actual Values (y_test)')\n","plt.ylabel('Predicted Values (y_pred)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"0BEtSeztxLCl","executionInfo":{"status":"aborted","timestamp":1715122234986,"user_tz":-180,"elapsed":38298,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLLomKNwK9hQ"},"source":["**MLPRegressor MLPRegressor MLPRegressor MLPRegressor MLPRegressor MLPRegressor MLPRegressor MLPRegressor**\n"]},{"cell_type":"markdown","source":["Hyperparameter tuning of MLPRegresso model"],"metadata":{"id":"7jbSEFFvOiCZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJ8ooatauzeC","executionInfo":{"status":"aborted","timestamp":1715122234987,"user_tz":-180,"elapsed":38296,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["\n","# MLPRegressor\n","\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","# oUTput data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Initialize MLP regressor\n","activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n","\n","# Initialize lists to store MSE values for training and test sets\n","train_mse_values = []\n","\n","Validation_mse_values = []\n","\n","# Loop over each solver\n","for activation in activation_functions:\n","    # Create MLPRegressor model with current activation\n","    mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50, 50), activation=activation, solver='adam',\n","                         alpha=0.0001, batch_size='auto', learning_rate='adaptive',\n","                         max_iter=1000, random_state=42)\n","\n","    # Train the regressor\n","    mlp_regressor.fit(X_train, y_train)\n","\n","\n","    # Predict on training and validation sets\n","\n","    y_train_pred = mlp_regressor.predict(X_train)\n","    y_test_pred = mlp_regressor.predict(X_validation)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred)\n","    test_mse = mean_squared_error(y_validation, y_test_pred)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","    print(f\"Activation_functions: {activation}, Train MSE: {train_mse}, Validation MSE: {test_mse}\")\n","\n","# Find the solver that minimizes the test MSE\n","best_Activation_functions = activation_functions[np.argmin(Validation_mse_values)]\n","\n","plt.figure(figsize=(50, 20))\n","\n","# Visualization\n","plt.plot(Validation_mse_values)\n","\n","print(f\"Best Activation functions: {best_Activation_functions}\")\n","\n","\n","\n"]},{"cell_type":"code","source":["# MLPRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","\n","# Output data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Initialize MLP regressor\n","solvers = ['lbfgs', 'sgd', 'adam']\n","\n","# Initialize lists to store MSE values for training and test sets\n","train_mse_values = []\n","\n","Validation_mse_values = []\n","\n","# Loop over each solver\n","for solver in solvers:\n","    # Create MLPRegressor model with current activation\n","    mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50, 50), activation=best_Activation_functions, solver=solver,\n","                         alpha=0.01, batch_size='auto', learning_rate='adaptive',\n","                         max_iter=1000, random_state=42)\n","    # Train the regressor\n","    mlp_regressor.fit(X_train, y_train)\n","\n","\n","    # Predict on training and validation sets\n","\n","    y_train_pred = mlp_regressor.predict(X_train)\n","    y_test_pred = mlp_regressor.predict(X_validation)\n","\n","    # Calculate MSE for training and test sets\n","    train_mse = mean_squared_error(y_train, y_train_pred)\n","    test_mse = mean_squared_error(y_validation, y_test_pred)\n","\n","    # Append MSE values to lists\n","    train_mse_values.append(train_mse)\n","    Validation_mse_values.append(test_mse)\n","\n","    print(f\"SolverS: {solver}, Train MSE: {train_mse}, Validation MSE: {test_mse}\")\n","\n","# Find the solver that minimizes the test MSE\n","best_solver = solvers[np.argmin(Validation_mse_values)]\n","plt.figure(figsize=(50, 20))\n","\n","# Visualization\n","plt.plot(Validation_mse_values)\n","\n","print(f\"Best best_solver: {best_solver }\")\n"],"metadata":{"id":"rwNeR1oJYWjt","executionInfo":{"status":"aborted","timestamp":1715122234987,"user_tz":-180,"elapsed":38294,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7T4gfetGPJhB","executionInfo":{"status":"aborted","timestamp":1715122234988,"user_tz":-180,"elapsed":38293,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"outputs":[],"source":["\n","# MLPRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Input data\n","X_train = train_data[:,1:]\n","\n","X_validation = val_data[:,1:]\n","\n","X_test = test_data[:,1:]\n","\n","# Output data\n","y_train = train_data[:,0]\n","\n","y_validation = val_data[:,0]\n","\n","y_test = test_data[:,0]\n","\n","# Initialize MLP regressor\n","mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50, 50), activation=best_Activation_functions, solver=best_solver ,\n","                         alpha=0.0001, batch_size='auto', learning_rate='adaptive',\n","                         max_iter=1000, random_state=42)\n","\n","# Train the regressor\n","mlp_regressor.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred_train_MLPR = mlp_regressor.predict(X_train) # Predict train set\n","\n","y_pred_test_MLPR = mlp_regressor.predict(X_test) # Predict train set\n","\n","# Calculate Mean Squared Error\n","mse_train  = mean_squared_error(y_train,y_pred_train_MLPR) # Calculate Mean Squared Error for train set\n","\n","mse_test = mean_squared_error(y_test, y_pred_test_MLPR) # Calculate Mean Squared Error for test set\n","\n","print(\"Mean Squared Error for train :\", mse_train ,\"Mean Squared Errorfor test:\", mse_test)"]},{"cell_type":"code","source":["# Visualization of MLPRegressor results\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","\n","# Scatter plot\n","plt.figure(figsize=(10, 6))\n","\n","plt.scatter(y_test,  y_pred_test_MLPR, color='blue', label='Data points')\n","\n","# Line indicating perfect predictions\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect predictions')\n","\n","plt.title('MLPRegressor results Actual vs Predicted')\n","plt.xlabel('Actual Values (y_test)')\n","plt.ylabel('Predicted Values (y_pred)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"mv9ziTNCvEmb","executionInfo":{"status":"aborted","timestamp":1715122234988,"user_tz":-180,"elapsed":38291,"user":{"displayName":"TStan TheMan","userId":"18026803421330775668"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1PdIbUahR84EmEblRSGiurENP8MI0iNPi","authorship_tag":"ABX9TyNkBYYfR43jOCYHhWN2b+l8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}